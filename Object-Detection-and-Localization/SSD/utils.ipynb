{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVsNIw_NJnyq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYF3StLqJxqs"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label map\n",
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
    "\n",
    "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
    "# This site mention color codes\n",
    "\n",
    "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
    "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
    "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
    "                   \n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqkcEe19vCRJ"
   },
   "outputs": [],
   "source": [
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-52u-lfiGyY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpdPr1n_iHs6"
   },
   "source": [
    "Below cell is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icVOoE5FJxxe"
   },
   "outputs": [],
   "source": [
    "#def create_data_lists(voc07_path, voc12_path, output_folder):\n",
    "def create_data_lists(voc07_path, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
    "\n",
    "    :param voc07_path: path to the 'VOC2007' folder\n",
    "    #NOT Included:param voc12_path: path to the 'VOC2012' folder\n",
    "    :param output_folder: folder where the JSONs must be saved\n",
    "    \"\"\"\n",
    "    voc07_path = os.path.abspath(voc07_path)\n",
    "    #voc12_path = os.path.abspath(voc12_path)\n",
    "\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Training data\n",
    "\n",
    "    #for path in [voc07_path, voc12_path]:\n",
    "    # for path in [voc07_path]:\n",
    "    #     # Find IDs of images in training data\n",
    "    #     with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "    #         ids = f.read().splitlines()\n",
    "\n",
    "    #     for id in ids:\n",
    "    #         # Parse annotation's XML file\n",
    "    #         objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "    #         if len(objects) == 0:\n",
    "    #             continue\n",
    "    #         n_objects += len(objects)\n",
    "    #         train_objects.append(objects)\n",
    "    #         train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
    "    \n",
    "    path_train=voc07_path+'/VOC_train_2007/'\n",
    "    # Find IDs of images in training data\n",
    "    with open(os.path.join(path_train, 'ImageSets/Main/trainval.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(path_train, 'Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        n_objects += len(objects)\n",
    "        train_objects.append(objects)\n",
    "        train_images.append(os.path.join(path_train, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(train_objects) == len(train_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "    # Validation data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    path_test = voc07_path+'/VOC_test_2007/'\n",
    "    # Find IDs of images in validation data\n",
    "    with open(os.path.join(path_test, 'ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(path_test, 'Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(path_test, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d validation images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tn2wXx2pJx4A"
   },
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZE0Qb1LJx7A"
   },
   "outputs": [],
   "source": [
    "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision (mAP) of detected objects.\n",
    "\n",
    "    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n",
    "\n",
    "    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n",
    "    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n",
    "    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n",
    "    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n",
    "    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n",
    "    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n",
    "    :return: list of average precisions for all classes, mean average precision (mAP)\n",
    "    \"\"\"\n",
    "    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n",
    "        true_labels) == len(\n",
    "        true_difficulties)  # these are all lists of tensors of the same length, i.e. number of images\n",
    "    n_classes = len(label_map)\n",
    "\n",
    "    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n",
    "    true_images = list()\n",
    "    for i in range(len(true_labels)):\n",
    "        true_images.extend([i] * true_labels[i].size(0))\n",
    "    true_images = torch.LongTensor(true_images).to(\n",
    "        device)  # (n_objects), n_objects is the total no. of objects across all images\n",
    "    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n",
    "    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n",
    "    true_difficulties = torch.cat(true_difficulties, dim=0)  # (n_objects)\n",
    "\n",
    "    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n",
    "\n",
    "    # Store all detections in a single continuous tensor while keeping track of the image it is from\n",
    "    det_images = list()\n",
    "    for i in range(len(det_labels)):\n",
    "        det_images.extend([i] * det_labels[i].size(0))\n",
    "    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n",
    "    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n",
    "    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n",
    "    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n",
    "\n",
    "    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n",
    "\n",
    "    # Calculate APs for each class (except background)\n",
    "    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n",
    "    for c in range(1, n_classes):\n",
    "        # Extract only objects with this class\n",
    "        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n",
    "        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n",
    "        true_class_difficulties = true_difficulties[true_labels == c]  # (n_class_objects)\n",
    "        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n",
    "\n",
    "        # Keep track of which true objects with this class have already been 'detected'\n",
    "        # So far, none\n",
    "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n",
    "            device)  # (n_class_objects)\n",
    "\n",
    "        # Extract only detections with this class\n",
    "        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n",
    "        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n",
    "        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n",
    "        n_class_detections = det_class_boxes.size(0)\n",
    "        if n_class_detections == 0:\n",
    "            continue\n",
    "\n",
    "        # Sort detections in decreasing order of confidence/scores\n",
    "        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n",
    "        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n",
    "        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n",
    "\n",
    "        # In the order of decreasing scores, check if true or false positive\n",
    "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
    "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
    "        for d in range(n_class_detections):\n",
    "            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n",
    "            this_image = det_class_images[d]  # (), scalar\n",
    "\n",
    "            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n",
    "            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n",
    "            object_difficulties = true_class_difficulties[true_class_images == this_image]  # (n_class_objects_in_img)\n",
    "            # If no such object in this image, then the detection is a false positive\n",
    "            if object_boxes.size(0) == 0:\n",
    "                false_positives[d] = 1\n",
    "                continue\n",
    "\n",
    "            # Find maximum overlap of this detection with objects in this image of this class\n",
    "            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n",
    "            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n",
    "\n",
    "            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n",
    "            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n",
    "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
    "            # We need 'original_ind' to update 'true_class_boxes_detected'\n",
    "\n",
    "            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n",
    "            if max_overlap.item() > 0.5:\n",
    "                # If the object it matched with is 'difficult', ignore it\n",
    "                if object_difficulties[ind] == 0:\n",
    "                    # If this object has already not been detected, it's a true positive\n",
    "                    if true_class_boxes_detected[original_ind] == 0:\n",
    "                        true_positives[d] = 1\n",
    "                        true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n",
    "                    # Otherwise, it's a false positive (since this object is already accounted for)\n",
    "                    else:\n",
    "                        false_positives[d] = 1\n",
    "            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n",
    "            else:\n",
    "                false_positives[d] = 1\n",
    "\n",
    "        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n",
    "        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n",
    "        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n",
    "        cumul_precision = cumul_true_positives / (\n",
    "                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n",
    "        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n",
    "\n",
    "        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n",
    "        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = cumul_recall >= t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = cumul_precision[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.\n",
    "        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n",
    "\n",
    "    # Calculate Mean Average Precision (mAP)\n",
    "    mean_average_precision = average_precisions.mean().item()\n",
    "\n",
    "    # Keep class-wise average precisions in a dictionary\n",
    "    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n",
    "\n",
    "    return average_precisions, mean_average_precision\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSqm80f8Jx_6"
   },
   "outputs": [],
   "source": [
    "def xy_to_cxcy(xy):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from boundary coordinates (x_min, y_min, x_max, y_max) to center-size coordinates (c_x, c_y, w, h).\n",
    "\n",
    "    :param xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
    "    :return: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
    "    \"\"\"\n",
    "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
    "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugynaQKZKzr1"
   },
   "outputs": [],
   "source": [
    "def cxcy_to_xy(cxcy):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n",
    "\n",
    "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
    "    :return: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
    "    \"\"\"\n",
    "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
    "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MF-RpAMKzqo"
   },
   "outputs": [],
   "source": [
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    \"\"\"\n",
    "    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n",
    "\n",
    "    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n",
    "    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n",
    "\n",
    "    In the model, we are predicting bounding box coordinates in this encoded form.\n",
    "\n",
    "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n",
    "    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n",
    "    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n",
    "    # See https://github.com/weiliu89/caffe/issues/155\n",
    "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
    "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZ3NeDfcKzmK"
   },
   "outputs": [],
   "source": [
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    \"\"\"\n",
    "    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n",
    "\n",
    "    They are decoded into center-size coordinates.\n",
    "\n",
    "    This is the inverse of the function above.\n",
    "\n",
    "    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n",
    "    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
    "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onpiqs3IKziM"
   },
   "outputs": [],
   "source": [
    "def find_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI8l8TyCKzdf"
   },
   "outputs": [],
   "source": [
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Find intersections\n",
    "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)\n",
    "\n",
    "\n",
    "# Some augmentation functions below have been adapted from\n",
    "# From https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hAzaKCUL9za"
   },
   "outputs": [],
   "source": [
    "def expand(image, boxes, filler):\n",
    "    \"\"\"\n",
    "    Perform a zooming out operation by placing the image in a larger canvas of filler material.\n",
    "\n",
    "    Helps to learn to detect smaller objects.\n",
    "\n",
    "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param filler: RBG values of the filler material, a list like [R, G, B]\n",
    "    :return: expanded image, updated bounding box coordinates\n",
    "    \"\"\"\n",
    "    # Calculate dimensions of proposed expanded (zoomed-out) image\n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    max_scale = 4\n",
    "    scale = random.uniform(1, max_scale)\n",
    "    new_h = int(scale * original_h)\n",
    "    new_w = int(scale * original_w)\n",
    "\n",
    "    # Create such an image with the filler\n",
    "    filler = torch.FloatTensor(filler)  # (3)\n",
    "    new_image = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(1).unsqueeze(1)  # (3, new_h, new_w)\n",
    "    # Note - do not use expand() like new_image = filler.unsqueeze(1).unsqueeze(1).expand(3, new_h, new_w)\n",
    "    # because all expanded values will share the same memory, so changing one pixel will change all\n",
    "\n",
    "    # Place the original image at random coordinates in this new image (origin at top-left of image)\n",
    "    left = random.randint(0, new_w - original_w)\n",
    "    right = left + original_w\n",
    "    top = random.randint(0, new_h - original_h)\n",
    "    bottom = top + original_h\n",
    "    new_image[:, top:bottom, left:right] = image\n",
    "\n",
    "    # Adjust bounding boxes' coordinates accordingly\n",
    "    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(\n",
    "        0)  # (n_objects, 4), n_objects is the no. of objects in this image\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtr3-JGzL-GS"
   },
   "outputs": [],
   "source": [
    "def random_crop(image, boxes, labels, difficulties):\n",
    "    \"\"\"\n",
    "    Performs a random crop in the manner stated in the paper. Helps to learn to detect larger and partial objects.\n",
    "\n",
    "    Note that some objects may be cut out entirely.\n",
    "\n",
    "    Adapted from https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
    "\n",
    "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
    "    :return: cropped image, updated bounding box coordinates, updated labels, updated difficulties\n",
    "    \"\"\"\n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    # Keep choosing a minimum overlap until a successful crop is made\n",
    "    while True:\n",
    "        # Randomly draw the value for minimum overlap\n",
    "        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])  # 'None' refers to no cropping\n",
    "\n",
    "        # If not cropping\n",
    "        if min_overlap is None:\n",
    "            return image, boxes, labels, difficulties\n",
    "\n",
    "        # Try up to 50 times for this choice of minimum overlap\n",
    "        # This isn't mentioned in the paper, of course, but 50 is chosen in paper authors' original Caffe repo\n",
    "        max_trials = 50\n",
    "        for _ in range(max_trials):\n",
    "            # Crop dimensions must be in [0.3, 1] of original dimensions\n",
    "            # Note - it's [0.1, 1] in the paper, but actually [0.3, 1] in the authors' repo\n",
    "            min_scale = 0.3\n",
    "            scale_h = random.uniform(min_scale, 1)\n",
    "            scale_w = random.uniform(min_scale, 1)\n",
    "            new_h = int(scale_h * original_h)\n",
    "            new_w = int(scale_w * original_w)\n",
    "\n",
    "            # Aspect ratio has to be in [0.5, 2]\n",
    "            aspect_ratio = new_h / new_w\n",
    "            if not 0.5 < aspect_ratio < 2:\n",
    "                continue\n",
    "\n",
    "            # Crop coordinates (origin at top-left of image)\n",
    "            left = random.randint(0, original_w - new_w)\n",
    "            right = left + new_w\n",
    "            top = random.randint(0, original_h - new_h)\n",
    "            bottom = top + new_h\n",
    "            crop = torch.FloatTensor([left, top, right, bottom])  # (4)\n",
    "\n",
    "            # Calculate Jaccard overlap between the crop and the bounding boxes\n",
    "            overlap = find_jaccard_overlap(crop.unsqueeze(0),\n",
    "                                           boxes)  # (1, n_objects), n_objects is the no. of objects in this image\n",
    "            overlap = overlap.squeeze(0)  # (n_objects)\n",
    "\n",
    "            # If not a single bounding box has a Jaccard overlap of greater than the minimum, try again\n",
    "            if overlap.max().item() < min_overlap:\n",
    "                continue\n",
    "\n",
    "            # Crop image\n",
    "            new_image = image[:, top:bottom, left:right]  # (3, new_h, new_w)\n",
    "\n",
    "            # Find centers of original bounding boxes\n",
    "            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.  # (n_objects, 2)\n",
    "\n",
    "            # Find bounding boxes whose centers are in the crop\n",
    "            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (\n",
    "                    bb_centers[:, 1] < bottom)  # (n_objects), a Torch uInt8/Byte tensor, can be used as a boolean index\n",
    "\n",
    "            # If not a single bounding box has its center in the crop, try again\n",
    "            if not centers_in_crop.any():\n",
    "                continue\n",
    "\n",
    "            # Discard bounding boxes that don't meet this criterion\n",
    "            new_boxes = boxes[centers_in_crop, :]\n",
    "            new_labels = labels[centers_in_crop]\n",
    "            new_difficulties = difficulties[centers_in_crop]\n",
    "\n",
    "            # Calculate bounding boxes' new coordinates in the crop\n",
    "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])  # crop[:2] is [left, top]\n",
    "            new_boxes[:, :2] -= crop[:2]\n",
    "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])  # crop[2:] is [right, bottom]\n",
    "            new_boxes[:, 2:] -= crop[:2]\n",
    "\n",
    "            return new_image, new_boxes, new_labels, new_difficulties\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_Jpj12VL-E_"
   },
   "outputs": [],
   "source": [
    "def flip(image, boxes):\n",
    "    \"\"\"\n",
    "    Flip image horizontally.\n",
    "\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :return: flipped image, updated bounding box coordinates\n",
    "    \"\"\"\n",
    "    # Flip image\n",
    "    new_image = FT.hflip(image)\n",
    "\n",
    "    # Flip boxes\n",
    "    new_boxes = boxes\n",
    "    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n",
    "    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n",
    "    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASLFQL7GL9_d"
   },
   "outputs": [],
   "source": [
    "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
    "    \"\"\"\n",
    "    Resize image. For the SSD300, resize to (300, 300).\n",
    "\n",
    "    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n",
    "    you may choose to retain them.\n",
    "\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n",
    "    \"\"\"\n",
    "    # Resize image\n",
    "    new_image = FT.resize(image, dims)\n",
    "\n",
    "    # Resize bounding boxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yr0TxENVL9-B"
   },
   "outputs": [],
   "source": [
    "def photometric_distort(image):\n",
    "    \"\"\"\n",
    "    Distort brightness, contrast, saturation, and hue, each with a 50% chance, in random order.\n",
    "\n",
    "    :param image: image, a PIL Image\n",
    "    :return: distorted image\n",
    "    \"\"\"\n",
    "    new_image = image\n",
    "\n",
    "    distortions = [FT.adjust_brightness,\n",
    "                   FT.adjust_contrast,\n",
    "                   FT.adjust_saturation,\n",
    "                   FT.adjust_hue]\n",
    "\n",
    "    random.shuffle(distortions)\n",
    "\n",
    "    for d in distortions:\n",
    "        if random.random() < 0.5:\n",
    "            if d.__name__ is 'adjust_hue':\n",
    "                # Caffe repo uses a 'hue_delta' of 18 - we divide by 255 because PyTorch needs a normalized value\n",
    "                adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
    "            else:\n",
    "                # Caffe repo uses 'lower' and 'upper' values of 0.5 and 1.5 for brightness, contrast, and saturation\n",
    "                adjust_factor = random.uniform(0.5, 1.5)\n",
    "\n",
    "            # Apply this distortion\n",
    "            new_image = d(new_image, adjust_factor)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9XK2r12UL98c"
   },
   "outputs": [],
   "source": [
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    \"\"\"\n",
    "    Apply the transformations above.\n",
    "\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
    "    :param split: one of 'TRAIN' or 'TEST', since different sets of transformations are applied\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
    "    \"\"\"\n",
    "    assert split in {'TRAIN', 'TEST'}\n",
    "\n",
    "    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n",
    "    # see: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "    # Skip the following operations if validation/evaluation\n",
    "    if split == 'TRAIN':\n",
    "        # A series of photometric distortions in random order, each with 50% chance of occurrence, as in Caffe repo\n",
    "        new_image = photometric_distort(new_image)\n",
    "\n",
    "        # Convert PIL image to Torch tensor\n",
    "        new_image = FT.to_tensor(new_image)\n",
    "\n",
    "        # Expand image (zoom out) with a 50% chance - helpful for training detection of small objects\n",
    "        # Fill surrounding space with the mean of ImageNet data that our base VGG was trained on\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
    "\n",
    "        # Randomly crop image (zoom in)\n",
    "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
    "                                                                         new_difficulties)\n",
    "\n",
    "        # Convert Torch tensor to PIL image\n",
    "        new_image = FT.to_pil_image(new_image)\n",
    "\n",
    "        # Flip image with a 50% chance\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = flip(new_image, new_boxes)\n",
    "\n",
    "    # Resize image to (300, 300) - this also converts absolute boundary coordinates to their fractional form\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))\n",
    "\n",
    "    # Convert PIL image to Torch tensor\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "\n",
    "    # Normalize by mean and standard deviation of ImageNet data that our base VGG was trained on\n",
    "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
    "\n",
    "    return new_image, new_boxes, new_labels, new_difficulties\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfwdCMTLKzYp"
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, scale):\n",
    "    \"\"\"\n",
    "    Scale learning rate by a specified factor.\n",
    "\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
    "    :param scale: factor to multiply learning rate with.\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * scale\n",
    "    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJD5Wg3qJx1t"
   },
   "outputs": [],
   "source": [
    "def accuracy(scores, targets, k):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy, from predicted and true labels.\n",
    "\n",
    "    :param scores: scores from the model\n",
    "    :param targets: true labels\n",
    "    :param k: k in top-k accuracy\n",
    "    :return: top-k accuracy\n",
    "    \"\"\"\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfjeR-E8M6FN"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, epochs_since_improvement, model, optimizer, loss, best_loss, is_best):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'epochs_since_improvement': epochs_since_improvement,\n",
    "             'loss': loss,\n",
    "             'best_loss': best_loss,\n",
    "             'model': model,\n",
    "             'optimizer': optimizer}\n",
    "    \n",
    "    filename = 'checkpoint_ssd300.pth.tar'\n",
    "    #torch.save(state, filename)\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        #best_filename_fullpath = r'/content/drive/My Drive/Computer_Vision_project/BEST_' + filename\n",
    "        torch.save(state, 'BEST_' + filename)\n",
    "        #torch.save(state, best_filename_fullpath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETFHlNu4M5-8"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpFU63CAM-OR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "utils.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
